{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f051853a-cdec-4fc0-a6e2-378f35f70a2b",
   "metadata": {},
   "source": [
    "1ans:\n",
    "\n",
    "The mathematical formula for a linear SVM (Support Vector Machine) is:\n",
    "\n",
    "h(x) = sign(w^T x + b)\n",
    "\n",
    "where:\n",
    "\n",
    "h(x) is the predicted class label for input vector x\n",
    "\n",
    "sign() is the sign function that returns +1 or -1 depending on the value of its argument\n",
    "\n",
    "w is the weight vector, which represents the hyperplane that separates the classes\n",
    "\n",
    "b is the bias term, which determines the position of the hyperplane in the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653495ab-991f-4300-be40-05ab3b55dd89",
   "metadata": {},
   "source": [
    "2ans:\n",
    "\n",
    "The objective function of a linear SVM (Support Vector Machine) is to find the optimal hyperplane that maximizes the margin between the two classes in the training data. The margin is the distance between the hyperplane and the closest data points of the two classes. The optimal hyperplane is the one that achieves the maximum margin while correctly classifying all training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecc952-6446-44ae-a2ec-81745b6f8ede",
   "metadata": {},
   "source": [
    "3ans:\n",
    "\n",
    "The kernel trick is a technique used in Support Vector Machines (SVMs) that allows them to perform non-linear classification tasks in a high-dimensional feature space without explicitly computing the coordinates of the data in that space. The basic idea behind the kernel trick is to replace the dot product in the feature space with a kernel function that computes a similarity measure between pairs of data points in the input space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486e290-35f6-4e6c-93c6-2e6cfd743146",
   "metadata": {},
   "source": [
    "4ans:\n",
    "\n",
    "in a Support Vector Machine (SVM), the support vectors are the data points that lie closest to the decision boundary (hyperplane) separating the two classes in the feature space. These data points are the most difficult to classify and therefore have the greatest influence on the position and orientation of the decision boundary.\n",
    "\n",
    "The SVM algorithm seeks to find the hyperplane that maximizes the margin between the two classes while correctly classifying all training examples. The margin is defined as the distance between the hyperplane and the closest data points of the two classes, which are the support vectors. The SVM algorithm aims to find the hyperplane that maximizes the margin, subject to the constraint that all training examples are correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31d066-97d5-48cc-aa43-75ffc53dbc4e",
   "metadata": {},
   "source": [
    "5ans:\n",
    "\n",
    "Hyperplane:\n",
    "\n",
    "In SVM, a hyperplane is a decision boundary that separates the data points of two classes in the feature space. The hyperplane is defined as a linear combination of the input variables and can be represented as:\n",
    "\n",
    "w*x + b = 0\n",
    "\n",
    "Marginal plane:\n",
    "\n",
    "In SVM, the marginal plane is the hyperplane that maximizes the margin between the two classes. The margin is defined as the distance between the hyperplane and the closest data points of the two classes, which are the support vectors. The marginal plane is found by solving an optimization problem that maximizes the margin subject to the constraint that all training examples are correctly classified.\n",
    "\n",
    "Soft margin:\n",
    "\n",
    "In SVM, the soft margin allows for some misclassification of the training examples to achieve better generalization performance on the test data. The soft margin is found by solving an optimization problem that maximizes the margin subject to a penalty term for misclassification errors.\n",
    "\n",
    "Hard margin:\n",
    "\n",
    "In SVM, the hard margin does not allow for any misclassification of the training examples and is appropriate when the data is linearly separable. The hard margin is found by solving an optimization problem that maximizes the margin subject to the constraint that all training examples are correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88858375-813a-4b3d-ad67-7054d3c87950",
   "metadata": {},
   "source": [
    "6ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
